# -*- coding: utf-8 -*-
"""AvaFeel_persian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ButjmAhvPm962966tv60jllkUkfuF_H
"""

import torch
import os
import numpy as np
from sklearn.metrics import accuracy_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""# speech to text"""

!pip install git+https://github.com/openai/whisper.git

import whisper

model_trans = whisper.load_model("medium")  # €åÿß "small", "medium", "large"

# import openai

# openai.api_key = "YOUR_API_KEY"

# audio_file = open("your_audio.mp3", "rb")

# transcript = openai.Audio.transcribe("whisper-1", audio_file, language="fa")

# print(transcript["text"])

"""# language model"""

!huggingface-cli login --token hf_....

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "meta-llama/Meta-Llama-3-8B" #"LLaMA-3-8b-base"
# model_name = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer_base = AutoTokenizer.from_pretrained(model_name)
model_base = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

"""# Data

ShEMO
"""

!pip install gdown

# ======================
# üß† DEFINE EMOTION MAP
# ======================
emotion_map = {
    'A': 'anger',
    'F': 'fear',
    'H': 'happiness',
    'N': 'neutral',
    'S': 'sadness',
    'W': 'surprise'
}

import os
import zipfile
import requests

def download_and_extract(url, base_output_dir, gender):
    filename = url.split('/')[-1]
    zip_path = os.path.join(base_output_dir, filename)
    gender_dir = os.path.join(base_output_dir, gender)

    os.makedirs(gender_dir, exist_ok=True)
    print(f"Downloading {filename}...")
    response = requests.get(url)
    with open(zip_path, 'wb') as f:
        f.write(response.content)
    print(f"Extracting {filename} into {gender_dir}...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(gender_dir)
    os.remove(zip_path)
    print(f"{filename} done!")

base_output_dir = './shemo_dataset'
os.makedirs(base_output_dir, exist_ok=True)
download_and_extract('https://github.com/mansourehk/ShEMO/raw/master/female.zip', base_output_dir, 'female')
download_and_extract('https://github.com/mansourehk/ShEMO/raw/master/male.zip', base_output_dir, 'male')
download_and_extract('https://github.com/mansourehk/ShEMO/raw/master/transcript.zip', base_output_dir, 'transcript')

import os
import pandas as pd

# ======================
# LOAD DATASET with TRANSCRIPTS
# ======================
root_dir = 'shemo_dataset'
transcript_dir = os.path.join(root_dir, 'transcript', 'final text')

paths = []
labels = []
transcripts = []

for gender in ['male', 'female']:
    folder = os.path.join(root_dir, gender)
    for file in os.listdir(folder):
        if file.endswith('.wav'):
            base_name = file.replace('.wav', '')
            emotion_code = file[3]
            emotion = emotion_map.get(emotion_code)
            if emotion:
                audio_path = os.path.join(folder, file)

                transcript_file = os.path.join(transcript_dir, f"{base_name}.ort")
                if os.path.exists(transcript_file):
                    with open(transcript_file, 'r', encoding='utf-8') as f:
                        transcript = f.read().strip()
                else:
                    transcript = None

                paths.append(audio_path)
                labels.append(emotion)
                transcripts.append(transcript)

df = pd.DataFrame({
    'audio_path': paths,
    'label': labels,
    'transcript': transcripts
})

print(df['label'].value_counts())
print(df.head())

"""# Fine-Tune LORA"""

from tqdm import tqdm

df_sample = df.sample(frac=0.5, random_state=1)
dataset_path = df_sample['audio_path']
labels = df_sample['label'].to_list()
transcribes = transcribes=df_sample['transcript'].to_list()

formatted_rows = []
i = 0
for path, transcribe in tqdm(zip(dataset_path, transcribes), total=len(dataset_path)):
  # try:
    if transcribe==None:
        # transcribe = model_trans.transcribe(path, language="fa")['text']
        transcribe = ''
    # prompt = create_prompt_base(transcribe, '/content/'+path)
    feature1, feature2, feature3 = extract_features('/content/'+path, transcript)
    prompt = create_prompt_svfs_context(transcribe, feature1, feature2, feature3)
    formatted_rows.append(prompt + " " + labels[i])
    i += 1
import json
# Save JSONL
jsonl_path = "/content/shemo_train_1500_lora.jsonl"
with open(jsonl_path, "w", encoding="utf-8") as f:
    for row in formatted_rows:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

!huggingface-cli login --token hf_...
!huggingface-cli whoami
!pip install -q transformers datasets accelerate peft
!pip install --upgrade bitsandbytes accelerate transformers peft
!pip install bitsandbytes

# import os
# os.kill(os.getpid(), 9)  # üîÅ Force restart kernel to clear bitsandbytes errors

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
from datasets import load_dataset, Dataset
import torch
import json
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# # Load dataset
# with open("/content/shemo_train_1500_lora.jsonl", "r") as f:
#     lines = [json.loads(l) for l in f]
# dataset = Dataset.from_list([{"text": line} for line in lines])


def extract_prompt_and_label(line):
    if "Emotion:" not in line:
        return None
    prompt, label = line.rsplit("Emotion:", 1)
    prompt = prompt.strip() + "Emotion:"
    label = label.strip()
    return prompt, label

jsonl_path = "/content/shemo_train_1500_lora.jsonl"

samples = []
with open(jsonl_path, "r", encoding="utf-8") as f:
    for l in f:
        raw_text = json.loads(l)
        text = raw_text["text"] if isinstance(raw_text, dict) else raw_text
        prompt, label = extract_prompt_and_label(text)
        full = f"<s>[INST] {prompt} [/INST] {label}</s>"
        samples.append({"text": full})

dataset = Dataset.from_list(samples)

# üß† Enable 4-bit quantization config
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "mistralai/Mistral-7B-Instruct-v0.3"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# Tokenize
def tokenize(example):
    return tokenizer(example['text'], truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize, batched=False)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    use_auth_token=True  # ‚¨ÖÔ∏è important!
)

# üìå Apply LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

# Training args
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=2,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    eval_steps=100,
    logging_steps=50,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    fp16=True,
    report_to="none"
)

# Data collator for causal LM
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# ‚úÖ Train
trainer.train()

# ‚úÖ Save
model.save_pretrained("/content/finetuned_llama3_lora")
tokenizer.save_pretrained("/content/finetuned_llama3_lora")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Set your repo path
repo_path = "nargesgholami/SED"

# Save and push model
model.push_to_hub(repo_path)
tokenizer.push_to_hub(repo_path)

"""#### load model"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# #new model loading

!huggingface-cli login --token hf_S...
!huggingface-cli whoami

model_id = "mistralai/Mistral-7B-Instruct-v0.3"

model_id = "mistralai/Mistral-7B-Instruct-v0.3"
adapter_id = "nargesgholami/SED2"

base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto"
)

model = PeftModel.from_pretrained(base_model, adapter_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

"""# extract features V2"""

!pip install pyloudnorm
!pip install praat-parselmouth

import os
import numpy as np
import pandas as pd
import librosa
import pyloudnorm as pyln
import parselmouth
from tqdm import tqdm
import re

def extract_last_emotion(prompt: str) -> str:
    label_set = {"anger", "fear", "joy", "neutral", "sadness", "surprise"}

    lines = prompt.strip().splitlines()
    emotion_lines = [line for line in lines if line.strip().lower().startswith("emotion:")]

    if not emotion_lines:
        return "None"

    raw = emotion_lines[-1].split("Emotion:", 1)[-1].strip()
    cleaned = re.sub(r"[^a-zA-Z]", "", raw).lower()
    cleaned = cleaned.strip('"').strip("'")

    if cleaned in label_set:
        return cleaned
    else:
        return "None"


def extract_features(path, transcript):
    try:
        y, sr = librosa.load(path, sr=None)
        duration = librosa.get_duration(y=y, sr=sr)
        if duration < 0.25 or np.max(np.abs(y)) < 1e-4:
            print(f"Skip (short/silent): {path}")
            return [np.nan] * 3

        snd = parselmouth.Sound(path)
        pitch = parselmouth.praat.call(snd, "To Pitch", 0.0, 75, 600)
        avg_pitch = parselmouth.praat.call(pitch, "Get mean", 0, 0, "Hertz")

        meter = pyln.Meter(sr)
        if len(y) < sr * 0.4:
            loudness = np.nan
        else:
            loudness = meter.integrated_loudness(y)
        num_words = len(transcript.split())
        num_syllables = num_words * 1.5
        speaking_rate = num_syllables / duration

        return avg_pitch, loudness, speaking_rate

    except Exception as e:
        print(f"Error {path}: {e}")
        return [np.nan] * 3


def categorize(value, low, high):
    if np.isnan(value):
        return "unknown"
    elif value < low:
        return "low"
    elif value < high:
        return "medium"
    else:
        return "high"

def describe_feature(pitch, loudness, speaking_rate):
    levels = {}
    thresholds = {
        "pitch": (146, 235),
        "loudness": (-23.3, -22),
        "speaking_rate": (3.4, 4.3),
    }
    levels["pitch"] = categorize(pitch, *thresholds["pitch"])
    levels["loudness"] = categorize(loudness, *thresholds["loudness"])
    levels["speaking_rate"] = categorize(speaking_rate, *thresholds["speaking_rate"])

    pitch_phrase = {
        "low": "a low pitch",
        "medium": "a medium pitch",
        "high": "a high pitch",
        "unknown": "an unknown pitch"
    }[levels["pitch"]]

    loudness_phrase = {
        "low": "a low volume",
        "medium": "a moderate tone",
        "high": "a loud tone",
        "unknown": "an unknown tone"
    }[levels["loudness"]]

    rate_phrase = {
        "low": "a slow pace",
        "medium": "a medium pace",
        "high": "a fast pace",
        "unknown": "an unknown pace"
    }[levels["speaking_rate"]]

    return f"The speaker speaks with {pitch_phrase}, {loudness_phrase}, and {rate_phrase}."


# Step 6: Create prompt with SVFS-lite and context
def create_prompt_svfs_context(context, pitch, loudness, speaking_rate):
    return f"""[INST]
You are an expert in detecting emotions from speech. Prioritize vocal cues such as pitch, loudness, and speaking rate, especially when the text is ambiguous.

Note that fast speaking rate or raised pitch may indicate excitement or urgency, not necessarily anger.
Always choose a **single dominant** emotion from the list. Do not list multiple emotions.
["happiness", "sadness", "anger", "fear", "surprise", "neutral"]

Do not invent new emotion labels.
Now classify the following example.

Conversation Context:
{context.strip()}

Speech Features:
{describe_feature(pitch, loudness, speaking_rate)}

Emotion: [/INST]
""".strip()


def create_prompt_svfs_context_few(context, pitch, loudness, speaking_rate):
    return f"""[INST]
You are an expert in detecting emotions from speech. Prioritize vocal cues such as pitch, loudness, and speaking rate, especially when the text is ambiguous.

Below are some examples of how to classify emotions from Farsi conversations using prosodic features.

Example 1:
Conversation Context:
ÿµÿ®ÿ± ⁄©ŸÜ... €å⁄©€å Ÿæÿ¥ÿ™ ÿØÿ±Ÿá! ÿ¥ŸÜ€åÿØ€å ÿµÿØÿßÿ¥Ÿàÿü ŸÅ⁄©ÿ± ⁄©ŸÜŸÖ ÿØÿßÿ±Ÿá ŸÖ€åÿßÿØ ÿß€åŸÜ‚Äåÿ∑ÿ±ŸÅ!

Speech Features:
The speaker speaks with a high pitch, a moderate tone, and a medium pace.

Emotion: anger

Example 2:
Conversation Context:
ÿµÿ®ÿ± ⁄©ŸÜ... €å⁄©€å Ÿæÿ¥ÿ™ ÿØÿ±Ÿá! ÿ¥ŸÜ€åÿØ€å ÿµÿØÿßÿ¥Ÿàÿü ŸÅ⁄©ÿ± ⁄©ŸÜŸÖ ÿØÿßÿ±Ÿá ŸÖ€åÿßÿØ ÿß€åŸÜ‚Äåÿ∑ÿ±ŸÅ!

Speech Features:
The speaker speaks with a low pitch, a moderate tone, and a medium pace.

Emotion: fear

Example 3:
Conversation Context:
ÿ®ÿßŸàÿ±ŸÖ ŸÜŸÖ€å‚Äåÿ¥Ÿá! ŸàŸÇÿ™€å ÿßÿ≥ŸÖŸÖŸà ÿÆŸàŸÜÿØŸÜÿå ŸáŸÖŸá ÿ¥ÿ±Ÿàÿπ ⁄©ÿ±ÿØŸÜ ÿØÿ≥ÿ™ ÿ≤ÿØŸÜ!

Speech Features:
The speaker speaks with a low pitch, a low volume, and a slow pace

Emotion: happiness


Now classify the following example.

Conversation Context:
{context.strip()}

Speech Features:
{describe_feature(pitch, loudness, speaking_rate)}

Note: fast speaking rate or raised pitch may indicate excitement or urgency, not necessarily anger.
Always choose a **single dominant** emotion from the list. Do not list multiple emotions.
["happiness", "sadness", "anger", "fear", "surprise", "neutral"]

Emotion: [/INST]
""".strip()

"""# evaluation"""

model.to(device)
# model_base.to(device)
# tokenizer_base.to(device)

"""#### translate"""

!pip install deep-translator

from deep_translator import GoogleTranslator

sentences_fa = df['transcript'].to_list()

sentences_en = []
for s in tqdm(sentences_fa):
    if isinstance(s, str) and s.strip() != "":
        try:
            translated = GoogleTranslator(source='fa', target='en').translate(s)
        except Exception:
            translated = "ERROR"
    else:
        translated = ""
    sentences_en.append(translated)

# sentences = sentences_en

# with open("translated_sentences.txt", "w", encoding="utf-8") as f:
#     for sentence in sentences:
#         f.write(sentence + "\n")
# from google.colab import files
# files.download("translated_sentences.txt")

# with open("translated_sentences.txt", "r", encoding="utf-8") as f:
#     sentences_en = [line.strip() for line in f]



"""#### eval"""

import pandas as pd
from sklearn.metrics import classification_report, accuracy_score


def accuracy(dataset_path, labels, transcribes=None, save_mistakes_path="/content/misclassified_few.csv"):
    label_map = {
        '0': 'anger',
        '1': 'fear',
        '2': 'happiness',
        '3': 'neutral',
        '4': 'sadness',
        '5': 'surprise'
    }
    text_to_label = {v: int(k) for k, v in label_map.items()}
    preds = []
    mistakes = []

    for i, (path, transcribe) in enumerate(tqdm(zip(dataset_path, transcribes), total=len(dataset_path))):
        if transcribe is None:
            transcribe = ''

        feature1, feature2, feature3 = extract_features('/content/' + path, transcribe)
        prompt = create_prompt_svfs_context_few(transcribe, feature1, feature2, feature3)
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        output = model.generate(**inputs, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)
        output_tokens = tokenizer.decode(output[0], skip_special_tokens=True)
        predicted_emotion = extract_last_emotion(output_tokens)
        # print("******", predicted_emotion, "\n_________")
        preds.append(predicted_emotion)

        true_label = labels[i]
        if predicted_emotion != true_label and len(mistakes) < 100:
            mistakes.append({
                "index": i,
                "audio_path": path,
                "transcript": transcribe,
                "true_label": true_label,
                "predicted_label": predicted_emotion,
                "prompt": prompt,
                "output": output_tokens.strip()
            })

        if len(preds) % 50 == 0:
            print(labels," : ",preds)

    print("\nClassification Report:")
    print(classification_report(labels, preds, labels=["anger", "fear", "happiness", "neutral", "sadness", "surprise"]))
    print("Accuracy:", accuracy_score(labels, preds))

    df = pd.DataFrame(mistakes)
    df.to_csv(save_mistakes_path, index=False, encoding="utf-8")
    print(f"\nüíæ Saved {len(mistakes)} misclassified examples to: {save_mistakes_path}")

import json
import re

used_transcripts = set()

with open("/content/shemo_train_1500_lora.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        row = json.loads(line)
        match = re.search(r'Conversation Context:\n(.*?)\n\nSpeech Features:', row, re.DOTALL)
        if match:
            transcript = match.group(1).strip()
            used_transcripts.add(transcript)
df_unused = df[~df['transcript'].isin(used_transcripts)]

"""few shot"""

n=len(df_unused)
accuracy(df_unused['audio_path'][:n], df_unused['label'].to_list()[:n], transcribes=df_unused['transcript'].to_list()[:n])

"""zero shot finetune shemo"""

n=len(df_unused)#new
accuracy(df_unused['audio_path'][:n], df_unused['label'].to_list()[:n], transcribes=df_unused['transcript'].to_list()[:n])
