# -*- coding: utf-8 -*-
"""Project-LLM-Avafeel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16hE1dNsUG5UsnYrksw0smPTf2c8mj3_L

### Prepare Train data
"""

!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True \
                   --to notebook --inplace /content/your_notebook.ipynb

# Step 1: Download and extract MELD if not done already
!wget -q --show-progress https://huggingface.co/datasets/declare-lab/MELD/resolve/main/MELD.Raw.tar.gz -O MELD.Raw.tar.gz
!tar -xzf MELD.Raw.tar.gz

# Step 2: Extract train audio files
!tar -xzf /content/MELD.Raw/train.tar.gz -C /content/MELD.Raw/

!tar -xzf /content/MELD.Raw/train.tar.gz -C /content/MELD.Raw/

import os
from glob import glob

video_dir = "/content/MELD.Raw/train_splits"
audio_dir = "/content/MELD.Raw/train_wav"
os.makedirs(audio_dir, exist_ok=True)

mp4_files = glob(os.path.join(video_dir, "*.mp4"))

from tqdm import tqdm
for mp4_path in tqdm(mp4_files, desc="Converting MP4 to WAV"):
    base = os.path.basename(mp4_path).replace(".mp4", ".wav")
    wav_path = os.path.join(audio_dir, base)
    # Use ffmpeg to convert (overwrite if exists)
    os.system(f'ffmpeg -y -i "{mp4_path}" -ar 16000 -ac 1 "{wav_path}" > /dev/null 2>&1')

import os
import pandas as pd
import numpy as np
import json
import librosa
import pyloudnorm as pyln
import parselmouth
from tqdm import tqdm

# Load the MELD training set
train_df = pd.read_csv("/content/MELD.Raw/train_sent_emo.csv")

# Limit to first 1000 rows for manageable fine-tuning
train_df = train_df.head(4000).copy()

# === Step 1: Extract Acoustic Features ===
def extract_features(path, transcript):
    try:
        y, sr = librosa.load(path, sr=None)
        duration = librosa.get_duration(y=y, sr=sr)
        if duration < 0.25 or np.max(np.abs(y)) < 1e-4:
            return [np.nan, np.nan, np.nan]

        snd = parselmouth.Sound(path)
        pitch = snd.to_pitch()
        f0 = pitch.selected_array['frequency']
        f0 = f0[f0 != 0]
        mean_pitch = np.mean(f0) if len(f0) > 0 else np.nan

        meter = pyln.Meter(sr)
        loudness = meter.integrated_loudness(y)

        num_words = len(transcript.split())
        num_syllables = num_words * 1.5
        speaking_rate = num_syllables / duration

        return [mean_pitch, loudness, speaking_rate]

    except Exception as e:
        return [np.nan, np.nan, np.nan]

# Build audio path column
audio_dir = "/content/MELD.Raw/train_wav"
def get_audio_path(row):
    fname = f"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}.wav"
    fpath = os.path.join(audio_dir, fname)
    return fpath if os.path.exists(fpath) else None

train_df["audio_path"] = train_df.apply(get_audio_path, axis=1)

# Extract features
acoustic_features = []
for _, row in tqdm(train_df.iterrows(), total=len(train_df)):
    feats = extract_features(row["audio_path"], row["Utterance"])
    acoustic_features.append(feats)

acoustic_df = pd.DataFrame(acoustic_features, columns=["pitch", "loudness", "speaking_rate"])
train_df = pd.concat([train_df.reset_index(drop=True), acoustic_df.reset_index(drop=True)], axis=1)

# === Step 2: Add Context Columns ===
def add_context_columns(df):
    df_sorted = df.sort_values(by=["Dialogue_ID", "Utterance_ID"]).reset_index(drop=True)
    df_sorted["prev_utt"] = df_sorted.groupby("Dialogue_ID")["Utterance"].shift(1).fillna("")
    df_sorted["next_utt"] = df_sorted.groupby("Dialogue_ID")["Utterance"].shift(-1).fillna("")
    df_sorted["prev_speaker"] = df_sorted.groupby("Dialogue_ID")["Speaker"].shift(1).fillna("")
    df_sorted["next_speaker"] = df_sorted.groupby("Dialogue_ID")["Speaker"].shift(-1).fillna("")
    return df_sorted

train_df = add_context_columns(train_df)

# === Step 3: Verbalize Features and Build Prompts ===
def verbalize_features(pitch, loudness, speaking_rate):
    thresholds = {
        "pitch": (194, 267),
        "loudness": (-31.50, -28.76),
        "speaking_rate": (2.86, 4.74),
    }
    features = {"pitch": pitch, "loudness": loudness, "speaking_rate": speaking_rate}
    phrases = []
    for k, v in features.items():
        if np.isnan(v):
            level = "unknown"
        elif v < thresholds[k][0]:
            level = "low"
        elif v < thresholds[k][1]:
            level = "medium"
        else:
            level = "high"
        if k == "pitch":
            phrases.append(f"a {level} pitch")
        elif k == "loudness":
            phrases.append(f"a {level} volume")
        elif k == "speaking_rate":
            phrases.append(f"a {level} pace")
    return "The speaker speaks with " + ", ".join(phrases) + "."

def build_prompt(row):
    context = ""
    if row["prev_utt"]:
        context += f'{row["prev_speaker"]}: "{row["prev_utt"]}"\n'
    context += f'Current speaker: "{row["Speaker"]}"\n'
    context += f'Utterance: "{row["Utterance"]}"\n'
    if row["next_utt"]:
        context += f'{row["next_speaker"]}: "{row["next_utt"]}"\n'

    acoustic = verbalize_features(row.get("pitch", np.nan), row.get("loudness", np.nan), row.get("speaking_rate", np.nan))

    prompt = f"""
You are an expert in detecting emotions from speech. Prioritize vocal cues such as pitch, loudness, and speaking rate, especially when the text is ambiguous.

Conversation Context:
{context.strip()}

Speech Features:
{acoustic}

Choose the dominant emotion of the current speaker from:
["joy", "sadness", "anger", "fear", "surprise", "disgust", "neutral"]

Emotion:
""".strip()
    return prompt

# Filter and build prompts
formatted_rows = []
for _, row in train_df.iterrows():
    if row["Emotion"] not in ["joy", "sadness", "anger", "fear", "surprise", "disgust", "neutral"]:
        continue
    prompt = build_prompt(row)
    response = row["Emotion"].strip().lower()
    formatted_rows.append({"prompt": prompt, "response": response})

# Save JSONL
jsonl_path = "/content/meld_train_1000_lora.jsonl"
with open(jsonl_path, "w", encoding="utf-8") as f:
    for row in formatted_rows:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

jsonl_path

train_df

"""### Prepare Test Data"""

# Step 1: Download and extract MELD dataset
!wget -q --show-progress https://huggingface.co/datasets/declare-lab/MELD/resolve/main/MELD.Raw.tar.gz -O MELD.Raw.tar.gz
!tar -xzf MELD.Raw.tar.gz
!tar -xzf /content/MELD.Raw/test.tar.gz -C /content/MELD.Raw/

# Step 2: Extract audio from MP4s
import os
from glob import glob
from tqdm import tqdm

input_dir = "/content/MELD.Raw/output_repeated_splits_test/"
mp4_files = glob(os.path.join(input_dir, "*.mp4"))

for mp4_path in tqdm(mp4_files):
    base = os.path.splitext(os.path.basename(mp4_path))[0]
    wav_path = os.path.join(input_dir, f"{base}.wav")
    os.system(f"ffmpeg -y -i '{mp4_path}' -ac 1 -ar 16000 '{wav_path}'")

# Step 3: Load test metadata and map to audio paths
import pandas as pd

test_df = pd.read_csv("/content/MELD.Raw/test_sent_emo.csv")
audio_dir = "/content/MELD.Raw/output_repeated_splits_test"

def get_audio_path(row):
    fname = f"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}.wav"
    fpath = os.path.join(audio_dir, fname)
    return fpath if os.path.exists(fpath) else None

test_df["audio_path"] = test_df.apply(get_audio_path, axis=1)

# Step 4: Add dialogue context (previous/next utterance)
def add_context_columns(df):
    df_sorted = df.sort_values(by=["Dialogue_ID", "Utterance_ID"]).reset_index(drop=True)
    df_sorted["prev_utt"] = df_sorted.groupby("Dialogue_ID")["Utterance"].shift(1)
    df_sorted["next_utt"] = df_sorted.groupby("Dialogue_ID")["Utterance"].shift(-1)
    df_sorted["prev_speaker"] = df_sorted.groupby("Dialogue_ID")["Speaker"].shift(1)
    df_sorted["next_speaker"] = df_sorted.groupby("Dialogue_ID")["Speaker"].shift(-1)
    return df_sorted.fillna("")

test_df = add_context_columns(test_df)

!pip install praat-parselmouth
!pip install pyloudnorm

import os
import numpy as np
import pandas as pd
import librosa
import pyloudnorm as pyln
import parselmouth
from tqdm import tqdm

# Step 1: Define feature extractor
def extract_features(path, transcript):
    try:
        y, sr = librosa.load(path, sr=None)
        duration = librosa.get_duration(y=y, sr=sr)
        if duration < 0.25 or np.max(np.abs(y)) < 1e-4:
            print(f"Skip (short/silent): {path}")
            return [np.nan] * 3

        snd = parselmouth.Sound(path)

        # Pitch via to_pitch
        pitch = parselmouth.praat.call(snd, "To Pitch", 0.0, 75, 600)
        avg_pitch = parselmouth.praat.call(pitch, "Get mean", 0, 0, "Hertz")

        # Loudness (only if long enough)
        meter = pyln.Meter(sr)
        if len(y) < sr * 0.4:
            loudness = np.nan
        else:
            loudness = meter.integrated_loudness(y)

        # Speaking rate
        num_words = len(transcript.split())
        num_syllables = num_words * 1.5
        speaking_rate = num_syllables / duration

        return [avg_pitch, loudness, speaking_rate]

    except Exception as e:
        print(f"Error {path}: {e}")
        return [np.nan] * 3



# Apply extraction
feature_columns = ["pitch", "loudness", "speaking_rate"]
features = []

for _, row in tqdm(test_df.iterrows(), total=len(test_df)):
    feats = extract_features(row["audio_path"], row["Utterance"])
    features.append(feats)

feature_df = pd.DataFrame(features, columns=feature_columns)
test_df = pd.concat([test_df, feature_df], axis=1)

print("MELD Acoustic Features", test_df)

test_df

test_df.to_csv("MELD_test_features.csv", index=False)

# Filter the DataFrame to include only pitch, loudness, and speaking_rate
selected_features = ["pitch", "loudness", "speaking_rate"]

# Compute 25th and 75th percentiles for selected features
thresholds_selected = {}
for feature in selected_features:
    values = test_df[feature].dropna()
    low = values.quantile(0.33)
    print("hi")
    print(low)
    high = values.quantile(0.66)
    thresholds_selected[feature] = (low, high)

thresholds_selected_df = pd.DataFrame(thresholds_selected, index=["low_threshold", "high_threshold"]).T
thresholds_selected_df

"""### LLama Model"""

!huggingface-cli login --token hf_S...

!huggingface-cli whoami

"""### Fine-tunining (LoRA)"""

!pip install -q transformers datasets accelerate peft

!pip install --upgrade bitsandbytes accelerate transformers peft

import os
os.kill(os.getpid(), 9)  # 🔁 Force restart kernel to clear bitsandbytes errors

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
from datasets import load_dataset, Dataset
import torch
import json
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training

# Load dataset
with open("/content/meld_train_4000_lora.jsonl", "r") as f:
    lines = [json.loads(l) for l in f]
dataset = Dataset.from_list(lines)

# 🧠 Enable 4-bit quantization config
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = "mistralai/Mistral-7B-Instruct-v0.3"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# Tokenize
def tokenize(example):
    prompt = example["prompt"] + " " + example["response"]
    return tokenizer(prompt, truncation=True, padding="max_length", max_length=512)

tokenized_dataset = dataset.map(tokenize, batched=False)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    use_auth_token=True  # ⬅️ important!
)

# 📌 Apply LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

# Training args
training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=2,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    eval_steps=100,
    logging_steps=50,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    fp16=True,
    report_to="none"
)

# Data collator for causal LM
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# ✅ Train
trainer.train()

# ✅ Save
model.save_pretrained("/content/finetuned_llama3_lora")
tokenizer.save_pretrained("/content/finetuned_llama3_lora")

from transformers import AutoTokenizer, AutoModelForCausalLM

# Set your repo path
repo_path = "nargesgholami/SED"

# Save and push model
model.push_to_hub(repo_path)
tokenizer.push_to_hub(repo_path)

"""### Prepare Prompt"""

import numpy as np
import librosa

def categorize(value, low, high):
    if np.isnan(value):
        return "unknown"
    elif value < low:
        return "low"
    elif value < high:
        return "medium"
    else:
        return "high"

def describe_feature(pitch, loudness, speaking_rate):
    levels = {}

    # Your learned thresholds
    thresholds = {
        "pitch": (194, 267),
        "loudness": (-31.504472, -28.763915),
        "speaking_rate": (2.869955, 4.738902),
    }

    # Categorize each
    levels["pitch"] = categorize(pitch, *thresholds["pitch"])
    levels["loudness"] = categorize(loudness, *thresholds["loudness"])
    levels["speaking_rate"] = categorize(speaking_rate, *thresholds["speaking_rate"])

    # Natural-language phrases
    pitch_phrase = {
        "low": "a low pitch",
        "medium": "a medium pitch",
        "high": "a high pitch",
        "unknown": "an unknown pitch"
    }[levels["pitch"]]

    loudness_phrase = {
        "low": "a low volume",
        "medium": "a moderate tone",
        "high": "a loud tone",
        "unknown": "an unknown tone"
    }[levels["loudness"]]

    rate_phrase = {
        "low": "a slow pace",
        "medium": "a medium pace",
        "high": "a fast pace",
        "unknown": "an unknown pace"
    }[levels["speaking_rate"]]

    return f"The speaker speaks with {pitch_phrase}, {loudness_phrase}, and {rate_phrase}."


# Step 6: Create prompt with SVFS-lite and context
def create_prompt_svfs_context(row):
    context = ""
    if pd.notna(row.get("next_utt")):
        context += f'{row["prev_speaker"]}: "{row["prev_utt"]}"\n'
    context += f'Current speaker: "{row["Speaker"]}"\n'
    context += f'Utterance: "{row["Utterance"]}"\n'
    if pd.notna(row.get("next_utt")):
        context += f'{row["next_speaker"]}: "{row["next_utt"]}"\n'

    return f"""
You are an expert in detecting emotions from speech. Prioritize vocal cues such as pitch, loudness, and speaking rate, especially when the text is ambiguous.

Conversation Context:
{context.strip()}

Speech Features:
{describe_feature(row['pitch'], row['loudness'], row['speaking_rate'])}

Note that fast speaking rate or raised pitch may indicate excitement or urgency, not necessarily anger.
Always choose a **single dominant** emotion from the list. Do not list multiple emotions.
["joy", "sadness", "anger", "fear", "surprise", "disgust", "neutral"]

Emotion:
""".strip()

# Step 8: Define prediction

def get_llm_prediction(model, tokenizer, prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # print("💕💕", decoded)
    return decoded.split("Emotion:")[-1].strip().split("\n")[0]

import pandas as pd
from tqdm import tqdm
test_df = pd.read_csv("MELD_test_features.csv")

# Step 9: Evaluate and save

def evaluate_and_save(model, tokenizer, df_chunks, base_name="meld_predictions"):
    for i, chunk in enumerate(df_chunks):
        predictions, truths, transcripts, audio_paths = [], [], [], []
        print(f"Processing chunk {i+1}/{len(df_chunks)}...")
        for _, row in tqdm(chunk.iterrows(), total=len(chunk)):
            prompt = create_prompt_svfs_context(row)
            pred = get_llm_prediction(model, tokenizer, prompt)
            print("\n pred is:", pred)
            print("Truth is:", row['Emotion'].lower())
            predictions.append(pred.lower())
            truths.append(row['Emotion'].lower())
            transcripts.append(row['Utterance'])
            audio_paths.append(row['audio_path'])

        pd.DataFrame({
            "transcript": transcripts,
            "audio_path": audio_paths,
            "true_emotion": truths,
            "predicted_emotion": predictions
        }).to_csv(f"{base_name}_chunk{i+1}.csv", index=False)

# Step 10: Run on 1 chunk for test
chunks = [test_df.iloc[:100]]  # replace 100 with len(test_df) // N later

evaluate_and_save(model, tokenizer, chunks)

import pandas as pd
import re
from sklearn.metrics import accuracy_score, classification_report

# Load predictions
df = pd.read_csv("meld_predictions_chunk1.csv")

# Drop rows where either true or predicted emotion is missing
df = df.dropna(subset=["true_emotion", "predicted_emotion"])

# Clean and normalize true labels
df["true_emotion"] = df["true_emotion"].astype(str).str.lower().str.strip()

# Function to extract the first valid emotion from predicted_emotion
def extract_first_emotion(s):
    s = str(s).lower().strip()
    # Remove enclosing brackets or parentheses
    s = re.sub(r'^[\[\(\"\']+|[\]\)\"\']+$', '', s)
    # Split on comma or space and return the first token that looks like a word
    parts = re.split(r'[,\s]+', s)
    for p in parts:
        if p in ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']:
            return p
    return s  # fallback (may be empty or malformed)

# Normalize predicted_emotion
df["predicted_emotion"] = df["predicted_emotion"].apply(extract_first_emotion)

# Extract cleaned values
y_true = df["true_emotion"]
y_pred = df["predicted_emotion"]

# Compute accuracy
acc = accuracy_score(y_true, y_pred)
print(f"Accuracy: {acc:.4f}")

# Show precision, recall, F1-score
labels = ["joy", "sadness", "anger", "fear", "surprise", "disgust", "neutral"]
print(classification_report(y_true, y_pred, labels=labels))

"""# ShEMO"""

!huggingface-cli login --token hf_S...

!huggingface-cli whoami

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

repo_path = "nargesgholami/SED"

base_model = AutoModelForCausalLM.from_pretrained(
    repo_path,

    torch_dtype="auto"
)

model_base = PeftModel.from_pretrained(base_model, repo_path)

tokenizer_base = AutoTokenizer.from_pretrained(repo_path)

!pip install pyloudnorm
!pip install praat-parselmouth

import librosa
import pyloudnorm as pyln
import parselmouth

def describe_feature(path):
    y, sr = librosa.load(path, sr=None)
    duration = librosa.get_duration(y=y, sr=sr)
    meter = pyln.Meter(sr, block_size=0.2)
    loudness = meter.integrated_loudness(y)

    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    pitch_values = pitches[magnitudes > np.median(magnitudes)]
    mean_pitch = np.mean(pitch_values)
    pitch_std = np.std(pitch_values)  # → pitch instability

    snd = parselmouth.Sound(path)
    point_process = parselmouth.praat.call(snd, "To PointProcess (periodic, cc)", 75, 500)
    jitter = parselmouth.praat.call(point_process, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
    shimmer = parselmouth.praat.call([snd, point_process], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)

    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()
    contrast = librosa.feature.spectral_contrast(y=y, sr=sr).mean()

    num_words = len(transcribe.split())
    num_syllables = num_words * 1.5
    speaking_rate = num_syllables / duration
    features = {
    "Pitch": [mean_pitch, [100, 200], ["low", "medium", "high"]],
    "Pitch Stability": [pitch_std, [10, 30], ["stable", "moderately variable", "unstable"]],
    "Loudness": [loudness, [-35, -20], ["soft", "normal", "loud"]],
    "Jitter": [jitter, [0.005, 0.01], ["stable", "slightly trembling", "highly unstable"]],
    "Shimmer": [shimmer, [0.03, 0.05], ["smooth", "mildly rough", "very rough"]],
    "Timbre": [centroid, [1500, 3000], ["dark", "balanced", "bright"]],
    "Speaking Rate": [speaking_rate, [4, 7], ["slow", "normal", "fast"]]
    }

    result = ''
    for name, info in features.items():
        value, thresholds, descriptions = info
        if value < thresholds[0]:
            desc = descriptions[0]
        elif value < thresholds[1]:
            desc = descriptions[1]
        else:
            desc = descriptions[2]
        result += f"- {name}: {desc}\n"
    return result

prompt_template_base = """
You are an expert emotion detector. Your task is to identify the dominant emotional tone of a given text.
Use both the meaning of the words and the vocal characteristics (volume, pitch, speed).

Only reply with the name of the dominant emotion. Do not add any explanation or example.

Choose from:
["joy", "sadness", "anger", "fear", "surprise", "disgust", "neutral"]

Example 1:
Transcript:
"I'm so excited to see you after all these years!"

Extracted audio features:
- Pitch: high
- Pitch Stability: moderately variable
- Loudness: loud
- Jitter: slightly trembling
- Shimmer: mildly rough
- Timbre: bright
- Speaking Rate: fast

Emotion: joy

Example 2:
Transcript:
"I don't know... I just feel like nothing matters anymore."

Extracted audio features:
- Pitch: low
- Pitch Stability: stable
- Loudness: soft
- Jitter: stable
- Shimmer: smooth
- Timbre: dark
- Speaking Rate: slow

Emotion: sadness

Now, analyze the following:

"{Transcript}"

Extracted audio features:
{Features}
Emotion:
""".strip()

def create_prompt_base(text, sound_path):
    return prompt_template_base.format(Transcript=text, Features=describe_feature(sound_path))

def extract_last_emotion(prompt: str) -> str:
    lines = prompt.strip().splitlines()
    emotion_lines = [line for line in lines if line.strip().startswith("Emotion:")]

    if not emotion_lines:
        return "No emotion found"

    last_emotion = emotion_lines[-1].strip().replace("Emotion:", "").strip()
    return last_emotion

# ======================
# 🧠 DEFINE EMOTION MAP
# ======================
emotion_map = {
    'A': 'anger',
    'F': 'fear',
    'H': 'happiness',
    'N': 'neutral',
    'S': 'sadness',
    'W': 'surprise'
}

import os
import zipfile
import requests

def download_and_extract(url, base_output_dir, gender):
    filename = url.split('/')[-1]
    zip_path = os.path.join(base_output_dir, filename)
    gender_dir = os.path.join(base_output_dir, gender)

    os.makedirs(gender_dir, exist_ok=True)

    # Download the file
    print(f"Downloading {filename}...")
    response = requests.get(url)
    with open(zip_path, 'wb') as f:
        f.write(response.content)

    # Extract into gender-specific folder
    print(f"Extracting {filename} into {gender_dir}...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(gender_dir)

    os.remove(zip_path)
    print(f"{filename} done!")

# Define base directory
base_output_dir = './shemo_dataset'
os.makedirs(base_output_dir, exist_ok=True)

# Download and extract into separate folders
download_and_extract('https://github.com/mansourehk/ShEMO/raw/master/female.zip', base_output_dir, 'female')
download_and_extract('https://github.com/mansourehk/ShEMO/raw/master/male.zip', base_output_dir, 'male')
download_and_extract('https://github.com/mansourehk/ShEMO/raw/master/transcript.zip', base_output_dir, 'transcript')

import json

with open('translated_text_a_path.json', 'r') as file:
    translated_text = json.load(file)

import os
import pandas as pd

# ======================
# 📂 LOAD DATASET with TRANSCRIPTS
# ======================
root_dir = 'shemo_dataset'
transcript_dir = os.path.join(root_dir, 'transcript', 'final text')

paths = []
labels = []
transcripts = []

for gender in ['male', 'female']:
    folder = os.path.join(root_dir, gender)
    for file in os.listdir(folder):
        if file.endswith('.wav'):
            base_name = file.replace('.wav', '')  # e.g., F01A01
            emotion_code = file[3]  # extract emotion character
            emotion = emotion_map.get(emotion_code)
            if emotion:
                audio_path = os.path.join(folder, file)

                # Match the transcript file
                transcript_file = os.path.join(transcript_dir, f"{base_name}.ort")
                if os.path.exists(transcript_file):
                    with open(transcript_file, 'r', encoding='utf-8') as f:
                        transcript = f.read().strip()
                else:
                    transcript = None  # fallback if not found

                paths.append(audio_path)
                labels.append(emotion)
                transcripts.append(transcript)

# Create the DataFrame
df = pd.DataFrame({
    'audio_path': paths,
    'label': labels,
    'transcript': transcripts
})

# Optional: show counts and sample rows
print(df['label'].value_counts())
print(df.head())

import pandas as pd
import json

df_json = pd.DataFrame(translated_text)

merged_df = pd.merge(df, df_json, on='audio_path', how='outer')

# ======================
# 🧾 MAP LABELS TO INTEGERS
# ======================
label_map = {label: idx for idx, label in enumerate(merged_df['label'].unique())}
inverse_label_map = {v: k for k, v in label_map.items()}
merged_df['label_cont'] = merged_df['label']
merged_df['label'] = merged_df['label'].map(label_map)

import os
import numpy as np
import torch
from torch.utils.data import Dataset
import librosa
import pandas as pd

class SpeechEmotionDataset(Dataset):
    def __init__(self, df, processor, max_length=48000):
        self.df = df.reset_index(drop=True)
        self.processor = processor
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        path = self.df.iloc[index]['audio_path']
        label = int(self.df.iloc[index]['label'])

        speech, _ = librosa.load(path, sr=16000)

        # Pad or truncate
        if len(speech) > self.max_length:
            speech = speech[:self.max_length]
        else:
            speech = np.pad(speech, (0, self.max_length - len(speech)))

        inputs = self.processor(
            speech,
            return_tensors='pt',
            sampling_rate=16000,
            padding=True,
            truncation=True,
            max_length=self.max_length
        )
        input_values = inputs.input_values.squeeze()

        return {
            'input_values': input_values,
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Example usage
#train_dataset = SpeechEmotionDataset(df, processor)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from tqdm import tqdm
from sklearn.metrics import accuracy_score
def accuracy(dataset_path, labels, transcribe):
  preds = []
  for i in tqdm(range(0,len(dataset_path))):
    path = dataset_path[i]
    trs = transcribe[i]
    try:
      prompt = create_prompt_base(trs, path)
      #print("******",prompt)
      inputs = tokenizer_base(prompt, return_tensors="pt")
      output = model_base.generate(**inputs, max_new_tokens=3, do_sample=False, pad_token_id=tokenizer_base.eos_token_id)
      output_tokens = tokenizer_base.decode(output[0], skip_special_tokens=True)
      preds.append(extract_last_emotion(output_tokens))
    except:
      preds.append(None)
  print(labels, "   :", preds)
  print(' ')
  print("accuracy: ", accuracy_score(labels, preds))
  return preds

dataset_path = merged_df['audio_path'].to_list()[20:40]
labels = merged_df['label'].to_list()[20:40]
trans = merged_df['translated_text'].to_list()[20:40]
lab_cont = merged_df['label_cont'].to_list()[20:40]

path = dataset_path[2]
transcribe = trans[2]
prompt = create_prompt_base(transcribe, path)
inputs = tokenizer_base(prompt, return_tensors="pt")
output = model_base.generate(**inputs, max_new_tokens=3, do_sample=False, pad_token_id=tokenizer_base.eos_token_id)
output_tokens = tokenizer_base.decode(output[0], skip_special_tokens=True)
extract_last_emotion(output_tokens)

per = accuracy(dataset_path, labels,trans)

print(lab_cont)

print(per)

from sklearn.metrics import accuracy_score, classification_report

accuracy_score(lab_cont, per)

import json

with open('label_20_40.json', 'w') as file:
    json.dump(per, file)





dataset_path = df['audio_path'].to_list()[60:80]
labels = df['label'].to_list()[60:80]
trans = df['transcript'].to_list()[60:80]
lab_cont = df['label_cont'].to_list()[60:80]



path = dataset_path[2]
transcribe = trans[2]
prompt = create_prompt_base(transcribe, path)
inputs = tokenizer_base(prompt, return_tensors="pt")
output = model_base.generate(**inputs, max_new_tokens=3, do_sample=False, pad_token_id=tokenizer_base.eos_token_id)
output_tokens = tokenizer_base.decode(output[0], skip_special_tokens=True)
extract_last_emotion(output_tokens)

per = accuracy(dataset_path, labels,trans)

print(per)

print(lab_cont)

import json

with open('label_60_80.json', 'w') as file:
    json.dump(per, file)

from sklearn.metrics import accuracy_score, classification_report

accuracy_score(lab_cont, per)

